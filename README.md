
# ü§ñ ChatBot com LLaMA e FastAPI

Este projeto √© uma API de chatbot constru√≠da com **FastAPI**, utilizando um modelo de linguagem grande (LLM) como o **LLaMA** via `transformers` da Hugging Face e suporte a quantiza√ß√£o com `bitsandbytes`. Ideal para uso em aplica√ß√µes com interface via Web, WhatsApp ou integra√ß√£o com outros servi√ßos.

---

## üì¶ Requisitos

- Python 3.10 ou superior
- CUDA (opcional, para uso com GPU NVIDIA)
- PyTorch
- `transformers`
- `bitsandbytes`
- `accelerate`
- `uvicorn`
- `fastapi`

Instale com:

```bash
pip install -r requirements.txt
```

---

## ‚öôÔ∏è Configura√ß√£o do modelo

Este projeto usa um modelo LLaMA ou similar com quantiza√ß√£o 4-bit para reduzir o uso de mem√≥ria.

**Aten√ß√£o:** Se sua GPU n√£o tiver mem√≥ria suficiente, o carregamento do modelo poder√° falhar. Duas abordagens est√£o dispon√≠veis:

### ‚úÖ Op√ß√£o 1: Carregar o modelo inteiramente na CPU (mais compat√≠vel, mais lento)

```python
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    quantization_config=bnb_config,
    device_map={"": "cpu"}
)
```

### ‚úÖ Op√ß√£o 2: Permitir offload entre GPU e CPU (requer `accelerate` configurado)

```python
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    quantization_config=bnb_config,
    device_map="auto",
    llm_int8_enable_fp32_cpu_offload=True
)
```

> Consulte [a documenta√ß√£o oficial](https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu) da Hugging Face para mais detalhes sobre offload.

---

## üöÄ Executando a API

Para rodar localmente com **FastAPI + Uvicorn**:

```bash
uvicorn main:app --reload
```

A aplica√ß√£o ser√° iniciada em: [http://127.0.0.1:8000](http://127.0.0.1:8000)

---

## üîç Testando a API

Ap√≥s iniciar a aplica√ß√£o, acesse a documenta√ß√£o interativa:

- Swagger: [http://127.0.0.1:8000/docs](http://127.0.0.1:8000/docs)
- Redoc: [http://127.0.0.1:8000/redoc](http://127.0.0.1:8000/redoc)

Voc√™ pode enviar mensagens ao chatbot via `POST`:

```json
POST /chat
{
  "message": "Ol√°, tudo bem?"
}
```

---

## üíª Exemplo de uso com interface web ou WhatsApp

Este back-end pode ser integrado a interfaces como:

- Front-end em **Gradio** ou **Streamlit**
- API para **WhatsApp** com **Twilio** ou **Venom Bot**
- Aplica√ß√µes m√≥veis via **Flutter**, **React Native** ou **Ionic**

---

## üß† Modelo utilizado

O modelo usado √© configur√°vel via Hugging Face. Exemplo:

```python
model_id = "TheBloke/Mistral-7B-Instruct-v0.2-GPTQ"
```

Voc√™ pode substituir por outros modelos compat√≠veis com quantiza√ß√£o, como:

- `meta-llama/Llama-2-7b-chat-hf`
- `mistralai/Mistral-7B-Instruct-v0.2`
- `tiiuae/falcon-7b-instruct`

---

## üõ†Ô∏è Dicas para uso eficiente

- Use modelos quantizados (`4bit`, `8bit`) para m√°quinas com pouca mem√≥ria.
- Use `device_map={"": "cpu"}` se n√£o tiver GPU ou quiser evitar erros.
- Certifique-se de que `accelerate` esteja instalado e atualizado.
